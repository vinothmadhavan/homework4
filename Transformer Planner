To implement the `TransformerPlanner` class, we will use a Transformer architecture where the waypoints are predicted using cross-attention between learned query embeddings and the lane boundary features. Here's how you can complete the code:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerPlanner(nn.Module):
    def __init__(
        self,
        n_track: int = 10,
        n_waypoints: int = 3,
        d_model: int = 64,
        nhead: int = 4,
        num_layers: int = 2,
    ):
        """
        Args:
            n_track (int): number of points in each side of the track
            n_waypoints (int): number of waypoints to predict
            d_model (int): dimension of the model
            nhead (int): number of attention heads
            num_layers (int): number of transformer decoder layers
        """
        super().__init__()

        self.n_track = n_track
        self.n_waypoints = n_waypoints
        self.d_model = d_model

        # Embedding for the query (waypoints)
        self.query_embed = nn.Embedding(n_waypoints, d_model)

        # Linear layer to project input features to d_model
        self.input_proj = nn.Linear(4, d_model)  # 4 because we concatenate left and right (2+2)

        # Transformer decoder
        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)

        # Output layer to project back to 2D waypoints
        self.output_proj = nn.Linear(d_model, 2)

    def forward(
        self,
        track_left: torch.Tensor,
        track_right: torch.Tensor,
        **kwargs,
    ) -> torch.Tensor:
        """
        Predicts waypoints from the left and right boundaries of the track.

        During test time, your model will be called with
        model(track_left=..., track_right=...), so keep the function signature as is.

        Args:
            track_left (torch.Tensor): shape (b, n_track, 2)
            track_right (torch.Tensor): shape (b, n_track, 2)

        Returns:
            torch.Tensor: future waypoints with shape (b, n_waypoints, 2)
        """
        # Concatenate the left and right track boundaries
        x = torch.cat((track_left, track_right), dim=2)  # Shape: (b, n_track, 4)

        # Project input to d_model
        x = self.input_proj(x)  # Shape: (b, n_track, d_model)

        # Prepare the query embeddings
        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, x.size(0), 1)  # Shape: (n_waypoints, b, d_model)

        # Transpose x to match the expected input shape for the transformer (n_track, b, d_model)
        x = x.permute(1, 0, 2)

        # Pass through the transformer decoder
        tgt = self.transformer_decoder(query_embed, x)  # Shape: (n_waypoints, b, d_model)

        # Transpose back to (b, n_waypoints, d_model)
        tgt = tgt.permute(1, 0, 2)

        # Project to 2D waypoints
        waypoints = self.output_proj(tgt)  # Shape: (b, n_waypoints, 2)

        return waypoints
```

### Explanation:
- **Initialization**: The `TransformerPlanner` class is initialized with parameters for the number of track points, waypoints, model dimension (`d_model`), number of attention heads (`nhead`), and number of transformer decoder layers (`num_layers`).
- **Query Embeddings**: We use `nn.Embedding` to create learned query embeddings for the waypoints.
- **Input Projection**: A linear layer projects the concatenated track boundary features to the model dimension (`d_model`).
- **Transformer Decoder**: The decoder uses cross-attention between the query embeddings and the projected track boundary features.
- **Output Projection**: A linear layer projects the decoder output to 2D waypoints.

This implementation provides a basic Transformer-based planner. You can experiment with different hyperparameters and architectures to optimize performance.
