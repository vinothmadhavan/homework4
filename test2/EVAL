[INFO     00:03:542] Transformer Planner
C:\Users\divya\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
[INFO     00:03:611]   - Test Output Shape                                  [ 5 / 5 ]
[WARNING  00:03:628]   - Longitudinal Error                                 [ 0 / 10 ]
[WARNING  00:03:628] Failed to load transformer_planner.th, make sure the default model arguments are set correctly
[WARNING  00:03:651]   - Longitudinal Error: Extra Credit                   [ 0 / 1 ]
[WARNING  00:03:651] Failed to load transformer_planner.th, make sure the default model arguments are set correctly
[WARNING  00:03:679]   - Lateral Error                                      [ 0 / 10 ]
[WARNING  00:03:680] Failed to load transformer_planner.th, make sure the default model arguments are set correctly
[WARNING  00:03:700]   - Lateral Error: Extra Credit                        [ 0 / 1 ]
[WARNING  00:03:700] Failed to load transformer_planner.th, make sure the default model arguments are set correctly
[WARNING  00:03:700]   - Driving Performance                                [ 0 / 10 ]
[WARNING  00:03:702] Skipping test (pystk not installed).
[INFO     00:03:702]  --------------------------------------------------    [   5 /  35 ]


class TransformerPlanner(nn.Module):
    def __init__(
        self,
        n_track: int = 10,
        n_waypoints: int = 3,
        d_model: int = 64,
        nhead: int = 8,  # Add nhead parameter
        num_layers: int = 6,  # Add num_layers parameter for the number of decoder layers
    ):
        super().__init__()

        self.n_track = n_track
        self.n_waypoints = n_waypoints
        self.d_model = d_model

        # Embedding for the query (waypoints)
        self.query_embed = nn.Embedding(n_waypoints, d_model)

        # Linear layer to project input features to d_model
        self.input_proj = nn.Linear(4, d_model)  # 4 because we concatenate left and right (2+2)

        # Transformer decoder
        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)

        # Output layer to project back to 2D waypoints
        self.output_proj = nn.Linear(d_model, 2)

    def forward(
        self,
        track_left: torch.Tensor,
        track_right: torch.Tensor,
        **kwargs,
    ) -> torch.Tensor:
        """
        Predicts waypoints from the left and right boundaries of the track.

        During test time, your model will be called with
        model(track_left=..., track_right=...), so keep the function signature as is.

        Args:
            track_left (torch.Tensor): shape (b, n_track, 2)
            track_right (torch.Tensor): shape (b, n_track, 2)

        Returns:
            torch.Tensor: future waypoints with shape (b, n_waypoints, 2)
        """

        # Concatenate the left and right track boundaries
        x = torch.cat((track_left, track_right), dim=2)  # Shape: (b, n_track, 4)

        # Project input to d_model
        x = self.input_proj(x)  # Shape: (b, n_track, d_model)

        # Prepare the query embeddings
        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, x.size(0), 1)  # Shape: (n_waypoints, b, d_model)

        # Transpose x to match the expected input shape for the transformer (n_track, b, d_model)
        x = x.permute(1, 0, 2)

        # Pass through the transformer decoder
        tgt = self.transformer_decoder(query_embed, x)  # Shape: (n_waypoints, b, d_model)

        # Transpose back to (b, n_waypoints, d_model)
        tgt = tgt.permute(1, 0, 2)

        # Project to 2D waypoints
        waypoints = self.output_proj(tgt)  # Shape: (b, n_waypoints, 2)

        return waypoints
    



